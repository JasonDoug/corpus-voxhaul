# Environment Configuration
NODE_ENV=development

# Local Development
PORT=3000
LOCAL_MODE=true

# AWS Configuration
AWS_REGION=us-west-2

# LocalStack Configuration (Development)
LOCALSTACK_ENDPOINT=http://localhost.localstack.cloud:4566
USE_LOCALSTACK=true

# S3 Configuration
S3_BUCKET_NAME=pdf-lecture-service
S3_PDF_PREFIX=pdfs
S3_AUDIO_PREFIX=audio
S3_CACHE_PREFIX=cache

# DynamoDB Configuration
DYNAMODB_JOBS_TABLE=pdf-lecture-jobs
DYNAMODB_AGENTS_TABLE=pdf-lecture-agents
DYNAMODB_CONTENT_TABLE=pdf-lecture-content

# Processing Configuration
MAX_PDF_SIZE_MB=100
ANALYSIS_TIMEOUT_MS=300000
AUDIO_SYNTHESIS_TIMEOUT_MS=600000

# External API Keys (Required for E2E testing)
# OpenRouter (recommended - unified access to multiple models)
OPENROUTER_API_KEY=sk-or-v1-56ffc60a283e33b659f8aa4790f376481156f65d109fa4b0d4d13614513b06f309fa4b0d4d13614513b06f3


# Or use direct API keys
OPENAI_API_KEY=
ANTHROPIC_API_KEY=

# Text-to-Speech Configuration
# Provider: 'mock' (default for dev) or 'polly' (AWS Polly)
TTS_PROVIDER=polly

# AWS Polly Configuration (only used if TTS_PROVIDER=polly)
# Engine options: generative, long-form, neural, standard
# - generative: Best quality, most natural (newest, limited voices, no word timings)
# - long-form: Optimized for long content like lectures (news-style voices)
# - neural: High quality, natural (most voices available, recommended)
# - standard: Basic quality, fast (all voices available, fallback)
POLLY_ENGINE=generative

# Note: AWS credentials are configured above in AWS Configuration section
# Polly uses the same AWS_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY

# LLM Provider (openrouter, openai, anthropic)
LLM_PROVIDER=openrouter

# Model Selection - Using FREE models from OpenRouter
# NOTE: With ENABLE_VISION_FIRST_PIPELINE=true, only VISION_MODEL and LLM_MODEL_SCRIPT are used
# - VISION_MODEL: Used for combined analysis + segmentation (vision-first pipeline)
# - LLM_MODEL_SCRIPT: Used for script generation with agent personality
# - LLM_MODEL_ANALYSIS and LLM_MODEL_SEGMENTATION are NOT used in vision-first mode

# Legacy models (only used if ENABLE_VISION_FIRST_PIPELINE=false)
LLM_MODEL_ANALYSIS=x-ai/grok-4.1-fast:free
LLM_MODEL_SEGMENTATION=x-ai/grok-4.1-fast:free

# Active models (used in vision-first pipeline)
LLM_MODEL_SCRIPT=tngtech/deepseek-r1t2-chimera:free

# Feature Flags - LLM Integration
ENABLE_REAL_SEGMENTATION=true
ENABLE_REAL_SCRIPT_GENERATION=true
ENABLE_IMAGE_EXTRACTION=true

# Vision-First Pipeline (RECOMMENDED)
ENABLE_VISION_FIRST_PIPELINE=true

# Vision Model Configuration
VISION_MODEL=nvidia/nemotron-nano-12b-v2-vl:free
VISION_LLM_TEMPERATURE=0.3
VISION_LLM_MAX_TOKENS=4000

# OpenRouter Rate Limiting Configuration
# Conservative settings for free tier to avoid rate limits
OPENROUTER_MIN_REQUEST_INTERVAL_MS=1000

# LLM Retry Configuration
# Aggressive retry settings to handle OpenRouter's flakiness
LLM_MAX_RETRY_ATTEMPTS=5
LLM_INITIAL_RETRY_DELAY_MS=2000
LLM_MAX_RETRY_DELAY_MS=30000

# Vision API Retry Configuration (even more aggressive)
LLM_VISION_MAX_RETRY_ATTEMPTS=5
LLM_VISION_INITIAL_RETRY_DELAY_MS=3000
LLM_VISION_MAX_RETRY_DELAY_MS=60000
