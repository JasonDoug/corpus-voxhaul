# Environment Configuration
NODE_ENV=development

# Local Development
PORT=3000
LOCAL_MODE=true

# AWS Configuration (Production)
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key

# LocalStack Configuration (Development)
LOCALSTACK_ENDPOINT=http://localhost:4566
USE_LOCALSTACK=true

# S3 Configuration
S3_BUCKET_NAME=pdf-lecture-service
S3_PDF_PREFIX=pdfs
S3_AUDIO_PREFIX=audio
S3_CACHE_PREFIX=cache

# DynamoDB Configuration
DYNAMODB_JOBS_TABLE=pdf-lecture-jobs
DYNAMODB_AGENTS_TABLE=pdf-lecture-agents
DYNAMODB_CONTENT_TABLE=pdf-lecture-content

# EventBridge Configuration
EVENT_BUS_NAME=pdf-lecture-service-events

# External API Keys
# OpenRouter (recommended - unified access to multiple models)
OPENROUTER_API_KEY=your_openrouter_key

# Or use direct API keys
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key

# Text-to-Speech Configuration
# Provider: 'mock' (for testing) or 'polly' (AWS Polly)
TTS_PROVIDER=mock

# AWS Polly Configuration (only used if TTS_PROVIDER=polly)
# Engine options: generative, long-form, neural, standard
# 
# Engine Comparison:
# - generative: Best quality, most natural sounding (newest)
#   * Pros: Most human-like, best for production
#   * Cons: Limited voice selection, no word-level timings (estimated instead)
#   * Availability: Limited regions (us-east-1, us-west-2, eu-west-1)
#   * Use for: Production lectures where quality > timing precision
#
# - long-form: Optimized for long content (news-style)
#   * Pros: Good for lectures, consistent over long text, word timings available
#   * Cons: Limited to news-style voices (Matthew, Joanna, etc.)
#   * Availability: Most regions
#   * Use for: Long lectures (>10 minutes), news-style delivery
#
# - neural: High quality, natural (RECOMMENDED)
#   * Pros: Great quality, most voices available, word timings available
#   * Cons: Not quite as natural as generative
#   * Availability: Most regions, most voices
#   * Use for: Production with precise word timing needs
#
# - standard: Basic quality, fast
#   * Pros: All voices available, fastest, cheapest
#   * Cons: Robotic sound, lower quality
#   * Availability: All regions, all voices
#   * Use for: Development, testing, cost optimization
#
POLLY_ENGINE=neural

# Voice Selection by Engine:
# Generative: Ruth, Stephen, Gregory, Burcu (limited but growing)
# Long-form: Matthew, Joanna, Lupe, Pedro (news-style)
# Neural: Most voices (Joanna, Matthew, Salli, Kendra, etc.)
# Standard: All voices

# Note: AWS credentials are configured in AWS Configuration section
# Polly uses: AWS_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY

# LLM Provider (openrouter, openai, anthropic)
# If not set, will auto-detect based on available API keys
LLM_PROVIDER=openrouter

# Model Selection (optional - uses recommended models if not set)
# For OpenRouter, see: https://openrouter.ai/models
#
# IMPORTANT: With ENABLE_VISION_FIRST_PIPELINE=true (recommended):
# - Only VISION_MODEL and LLM_MODEL_SCRIPT are used
# - VISION_MODEL handles combined analysis + segmentation in one step
# - LLM_MODEL_SCRIPT handles script generation with agent personality
# - LLM_MODEL_ANALYSIS and LLM_MODEL_SEGMENTATION are ignored
#
# If ENABLE_VISION_FIRST_PIPELINE=false (legacy multi-step pipeline):
# - LLM_MODEL_ANALYSIS: Text extraction and element detection
# - LLM_MODEL_VISION: Figure/table/formula analysis (not used in vision-first)
# - LLM_MODEL_SEGMENTATION: Content organization into segments
# - LLM_MODEL_SCRIPT: Script generation

# Legacy models (only used if vision-first pipeline is disabled)
LLM_MODEL_ANALYSIS=openai/gpt-4-turbo-preview
LLM_MODEL_SEGMENTATION=anthropic/claude-3-opus

# Active models (used in vision-first pipeline - RECOMMENDED)
LLM_MODEL_SCRIPT=openai/gpt-4-turbo-preview

# Processing Configuration
MAX_PDF_SIZE_MB=100
ANALYSIS_TIMEOUT_MS=300000
AUDIO_SYNTHESIS_TIMEOUT_MS=600000

# Feature Flags - LLM Integration Rollout
# Set to 'true' to enable real LLM API calls, 'false' to use mock implementations
ENABLE_REAL_SEGMENTATION=true
ENABLE_REAL_SCRIPT_GENERATION=true
ENABLE_IMAGE_EXTRACTION=true

# Vision-First Pipeline (RECOMMENDED)
# Set to 'true' to use simplified vision-first pipeline
# This replaces the complex multi-step analyzer + segmenter with a single vision LLM call per page
ENABLE_VISION_FIRST_PIPELINE=true

# Vision Model Configuration
# Default uses free Gemini model, but you can use any vision-capable model
VISION_MODEL=google/gemini-2.0-flash-exp:free
# Alternatives:
# VISION_MODEL=anthropic/claude-3-5-sonnet
# VISION_MODEL=openai/gpt-4-vision-preview
# VISION_MODEL=google/gemini-pro-vision

# Vision LLM Settings
VISION_LLM_TEMPERATURE=0.3
VISION_LLM_MAX_TOKENS=4000

# OpenRouter Rate Limiting Configuration
# Minimum interval between requests in milliseconds (default: 500ms for free tier)
# Increase this if you're hitting rate limits frequently
OPENROUTER_MIN_REQUEST_INTERVAL_MS=500

# LLM Retry Configuration
# Maximum number of retry attempts for LLM calls
LLM_MAX_RETRY_ATTEMPTS=5
LLM_INITIAL_RETRY_DELAY_MS=2000
LLM_MAX_RETRY_DELAY_MS=30000

# Vision API Retry Configuration (more aggressive for rate-limited vision APIs)
LLM_VISION_MAX_RETRY_ATTEMPTS=5
LLM_VISION_INITIAL_RETRY_DELAY_MS=3000
LLM_VISION_MAX_RETRY_DELAY_MS=60000
